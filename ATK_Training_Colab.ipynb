{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {"provenance": [], "gpuType": "T4"},
    "kernelspec": {"name": "python3", "display_name": "Python 3"},
    "accelerator": "GPU"
  },
  "cells": [
    {"cell_type": "markdown", "source": ["# ATK Classifier Training\n", "Upload ZIP dataset, train model, download untuk Streamlit"], "metadata": {}},
    {"cell_type": "markdown", "source": ["## 1. Upload Dataset ZIP\n", "ZIP folder dataset_alat_tulis lalu upload"], "metadata": {}},
    {"cell_type": "code", "source": ["from google.colab import files\nimport zipfile, os\n\nprint('Upload dataset_alat_tulis.zip...')\nuploaded = files.upload()\n\nfor f in uploaded.keys():\n    with zipfile.ZipFile(f, 'r') as z:\n        z.extractall('.')\n    print(f'Extracted {f}')\n\nDATASET_DIR = 'dataset_alat_tulis'\nif os.path.exists(DATASET_DIR):\n    for c in os.listdir(DATASET_DIR):\n        print(f'{c}: {len(os.listdir(os.path.join(DATASET_DIR, c)))} images')"], "metadata": {}, "execution_count": null, "outputs": []},
    {"cell_type": "markdown", "source": ["## 2. Setup"], "metadata": {}},
    {"cell_type": "code", "source": ["import os, json, cv2, imghdr\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom datetime import datetime\n\nIMG_SIZE = 300\nBATCH_SIZE = 15\nEPOCHS = 15\nLR = 0.001\nDATASET_DIR = 'dataset_alat_tulis'\n\nprint(f'TF: {tf.__version__}')\nprint(f'GPU: {tf.config.list_physical_devices(\"GPU\")}')"], "metadata": {}, "execution_count": null, "outputs": []},
    {"cell_type": "markdown", "source": ["## 3. Clean Images"], "metadata": {}},
    {"cell_type": "code", "source": ["exts = ['jpeg', 'jpg', 'png']\nremoved = valid = 0\nfor cls in os.listdir(DATASET_DIR):\n    p = os.path.join(DATASET_DIR, cls)\n    if os.path.isdir(p):\n        for img in os.listdir(p):\n            ip = os.path.join(p, img)\n            try:\n                if cv2.imread(ip) is None or imghdr.what(ip) not in exts:\n                    os.remove(ip); removed += 1\n                else: valid += 1\n            except: os.remove(ip); removed += 1\nprint(f'Valid: {valid}, Removed: {removed}')"], "metadata": {}, "execution_count": null, "outputs": []},
    {"cell_type": "markdown", "source": ["## 4. Load Data"], "metadata": {}},
    {"cell_type": "code", "source": ["train_ds = tf.keras.utils.image_dataset_from_directory(\n    DATASET_DIR, validation_split=0.1, subset='training',\n    seed=123, image_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE)\nval_ds = tf.keras.utils.image_dataset_from_directory(\n    DATASET_DIR, validation_split=0.1, subset='validation',\n    seed=123, image_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE)\n\nclass_names = train_ds.class_names\nprint(f'Classes: {class_names}')\n\nAUTO = tf.data.AUTOTUNE\ntrain_ds = train_ds.cache().shuffle(1000).prefetch(AUTO)\nval_ds = val_ds.cache().prefetch(AUTO)"], "metadata": {}, "execution_count": null, "outputs": []},
    {"cell_type": "markdown", "source": ["## 5. Build Model"], "metadata": {}},
    {"cell_type": "code", "source": ["model = models.Sequential([\n    layers.Rescaling(1./255, input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n    layers.Conv2D(32, 3, padding='same', activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Conv2D(64, 3, padding='same', activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Conv2D(128, 3, padding='same', activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(len(class_names), activation='softmax')\n])\nmodel.compile(optimizer=optimizers.Adam(LR),\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n              metrics=['accuracy'])\nmodel.summary()"], "metadata": {}, "execution_count": null, "outputs": []},
    {"cell_type": "markdown", "source": ["## 6. Train"], "metadata": {}},
    {"cell_type": "code", "source": ["MODEL_PATH = 'best_model.keras'\ncallbacks = [\n    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n    ModelCheckpoint(MODEL_PATH, monitor='val_accuracy', save_best_only=True)\n]\nhistory = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=callbacks)\nprint('Done!')"], "metadata": {}, "execution_count": null, "outputs": []},
    {"cell_type": "markdown", "source": ["## 7. Results"], "metadata": {}},
    {"cell_type": "code", "source": ["acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nplt.plot(acc, label='Train')\nplt.plot(val_acc, label='Val')\nplt.title('Accuracy'); plt.legend()\nplt.subplot(1,2,2)\nplt.plot(loss, label='Train')\nplt.plot(val_loss, label='Val')\nplt.title('Loss'); plt.legend()\nplt.show()\n\nprint(f'Accuracy: {acc[-1]:.2%}')\nprint(f'Val Accuracy: {val_acc[-1]:.2%}')"], "metadata": {}, "execution_count": null, "outputs": []},
    {"cell_type": "markdown", "source": ["## 8. Confusion Matrix"], "metadata": {}},
    {"cell_type": "code", "source": ["y_true, y_pred = [], []\nfor imgs, lbls in val_ds:\n    preds = model.predict(imgs, verbose=0)\n    y_true.extend(lbls.numpy())\n    y_pred.extend(np.argmax(preds, axis=1))\n\ncm = confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(8,6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=class_names, yticklabels=class_names)\nplt.xlabel('Pred'); plt.ylabel('True')\nplt.show()\nprint(classification_report(y_true, y_pred, target_names=class_names))"], "metadata": {}, "execution_count": null, "outputs": []},
    {"cell_type": "markdown", "source": ["## 9. Save & Download"], "metadata": {}},
    {"cell_type": "code", "source": ["META_PATH = 'best_model.json'\nmeta = {\n    'class_names': class_names,\n    'input_size': [IMG_SIZE, IMG_SIZE],\n    'metrics': {'accuracy': float(acc[-1]), 'val_accuracy': float(val_acc[-1])},\n    'timestamp': datetime.now().isoformat()\n}\nwith open(META_PATH, 'w') as f:\n    json.dump(meta, f, indent=2)\nprint(f'Saved: {MODEL_PATH}, {META_PATH}')"], "metadata": {}, "execution_count": null, "outputs": []},
    {"cell_type": "code", "source": ["from google.colab import files\nfiles.download(MODEL_PATH)\nfiles.download(META_PATH)\nprint('\\nPindahkan ke folder models/ lalu jalankan:')\nprint('streamlit run app/main.py')"], "metadata": {}, "execution_count": null, "outputs": []}
  ]
}
